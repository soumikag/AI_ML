{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes Classifier\n",
    "\n",
    "This Naive Bayes Classifier is trained on the UCI breast cancer dataset [1], and predicts recurrence based on patient age, menopausal status, tumor size, number of INV nodes, node caps, degree of malignancy, breast side, breast quadrant,  and irradiation. \n",
    "\n",
    "It is evaluated via cross validation with error rate as the evaluation metric. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = np.genfromtxt(\"breast+cancer/breast-cancer.data\", delimiter = \",\", \n",
    "                        dtype = \"str\").astype(str)\n",
    "dataset = dataset[np.all(dataset != '?', axis=1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get_att_vals\n",
    "* Takes in dataset\n",
    "* Returns a dict of attributes and their potential values as a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_att_vals(data):\n",
    "    att_vals = {}\n",
    "    for att in range(len(data[0])):\n",
    "        vals = np.unique(data[:, att])\n",
    "        att_vals[att] = vals\n",
    "    return(att_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_data = np.asarray([['a', 'c', 'b'], ['b', 'a', 'b']])\n",
    "test_att_vals = get_att_vals(test_data)\n",
    "assert(np.array_equal(test_att_vals[0],['a', 'b']))\n",
    "assert(np.array_equal(test_att_vals[1],['a', 'c']))\n",
    "assert(np.array_equal(test_att_vals[2],['b']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get_att_counts\n",
    "* Takes in a dict of attribute to values list, dataset as numpy 2d array, and smoothing as a bool)\n",
    "* Returns a nested dict, where each attribute maps to a dict of potential labels as keys and counts as values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_att_counts(att_vals, data, smoothing):\n",
    "    counts = {}\n",
    "    for att, vals in att_vals.items():\n",
    "        att_col = data[:, att]\n",
    "        val_counts = {}\n",
    "        for val in vals:\n",
    "            count = (att_col==val).sum()\n",
    "            if smoothing: count += 1\n",
    "            val_counts[val] = count\n",
    "        counts[att] = val_counts\n",
    "    return counts #counts[attribute][value] = the occurences of 'value' for given attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_att_counts = get_att_counts(test_att_vals, test_data, smoothing=False)\n",
    "assert(len(test_att_counts) == 3)\n",
    "assert(test_att_counts[0]['a'] == 1)\n",
    "test_smooth_att_counts = get_att_counts(test_att_vals, test_data, smoothing=True)\n",
    "assert(test_smooth_att_counts[2]['b'] == 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get_att_probs\n",
    "* Takes in the nested dict of attributes and value counts\n",
    "* Returns a nested dict of attributes and probabilities of each value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_att_probs(att_counts):\n",
    "    att_probs = {}\n",
    "    for attribute, counts in att_counts.items():\n",
    "        val_probs = {}\n",
    "        total = sum(counts.values())\n",
    "        for value, count in counts.items():\n",
    "            prob = float(count/total)\n",
    "            val_probs[value] = prob\n",
    "        att_probs[attribute] = val_probs\n",
    "    return att_probs #att_probs[attribute][value] = prob of value occuring for given att"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_att_probs = get_att_probs(test_att_counts)\n",
    "assert(len(test_att_probs) == 3)\n",
    "assert(test_att_probs[0] == {'a': 0.5, 'b': 0.5})\n",
    "assert(test_att_probs[2]['b'] == 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get_intersect_prob\n",
    "* Takes in two attributes, and their corresponding values, as well as the dataset\n",
    "* Returns the probability of intersection of the two values for the given attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_intersect_prob(att_1, val_1, att_2, val_2, data):\n",
    "    intersect = data[(data[:, att_1] == val_1) & (data[:, att_2] == val_2)]\n",
    "    return float(len(intersect)/len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_intersect_prob = get_intersect_prob(0, 'a', 2, 'b', test_data)\n",
    "assert(test_intersect_prob == 0.5)\n",
    "test_intersect_prob = get_intersect_prob(0, 'b', 2, 'c', test_data)\n",
    "assert(test_intersect_prob == 0)\n",
    "test_intersect_prob = get_intersect_prob(1, 'c', 2, 'b', test_data)\n",
    "assert(test_intersect_prob == 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get_cond_probs\n",
    "* Takes in the nested dict of attributes and value porbabilities, attributes, and data\n",
    "* Returns the conditional probabilities of each x|y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cond_probs(att_probs, atts, data): #calculate x|y\n",
    "    probs = {}\n",
    "    for label in atts[0]: #our 'x' values\n",
    "        att_cond_probs = {}\n",
    "        for att in range(1,len(atts)):\n",
    "            val_probs = {}\n",
    "            for val in atts[att]:\n",
    "                if att_probs[att][val] == 0: prob = 0\n",
    "                else:\n",
    "                    prob = float(get_intersect_prob(0, label, att, val, data)/\n",
    "                             att_probs[att][val])\n",
    "                val_probs[val] = prob\n",
    "            att_cond_probs[att] = val_probs\n",
    "        probs[label] = att_cond_probs\n",
    "    return probs #probs[label][att][val] = p(label|val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_cond_probs = get_cond_probs(test_att_probs, test_att_vals, test_data)\n",
    "assert(test_cond_probs['a'][2]['b'] == 0.5)\n",
    "assert(test_cond_probs['b'][1]['c'] == 0)\n",
    "assert(test_cond_probs['b'][1]['a'] == 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train \n",
    "* Takes in data, the dict of attributes and values, and smoothing as a boolean\n",
    "* Returns the model which is a set of conditional probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(data, att_vals, smoothing = True):\n",
    "    att_counts = get_att_counts(att_vals, data, smoothing)\n",
    "    att_probs = get_att_probs(att_counts)\n",
    "    cond_probs = get_cond_probs(att_probs, att_vals, data)\n",
    "    return(cond_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model = train(test_data, test_att_vals)\n",
    "assert(test_model['a'][2]['b'] == 0.5)\n",
    "assert(test_model['a'][1]['c'] == 1)\n",
    "assert(test_model['b'][1]['c'] == 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get_prob\n",
    "* Takes in the model as a triple nested dict, instance as a list of attributes, and label we are calculating probability for\n",
    "* Returns the probability of that label for the given instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prob(model, instance, label):\n",
    "    probs = []\n",
    "    for att in range(len(instance)-1):\n",
    "        val = instance[att]\n",
    "        prob = model[label][att+1][val]\n",
    "        probs.append(prob)\n",
    "    return(np.prod(probs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(get_prob(test_model, ['c','b'], 'a') == 1)\n",
    "assert(get_prob(test_model, ['a','b'], 'a') == 0)\n",
    "assert(get_prob(test_model, ['a','b'], 'b') == 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## normalize\n",
    "* Takes in results as a dict of label keys and probability values\n",
    "* Returns a dict of label keys and normalized probabilitiy values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(results):\n",
    "    normalized = {}\n",
    "    total = float(sum(results.values()))\n",
    "    for label, prob in results.items():\n",
    "        if(total == 0): normalized[label] = 0\n",
    "        else: normalized[label] = float(prob/total)\n",
    "    return normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(normalize({\"0\": 0.01, \"1\": 0.01}) == {\"0\": 0.5, \"1\": 0.5})\n",
    "assert(normalize({\"0\": 0.01, \"1\": 0.03}) == {\"0\": 0.25, \"1\": 0.75})\n",
    "assert(normalize({\"0\": 0.00, \"1\": 0.01}) == {\"0\": 0, \"1\": 1})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## classify_instance\n",
    "* Takes in a model as a triple nested dict and an instance as a list of attributes\n",
    "* Returns a tuple of the best label, and a dict of label/probability key-values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_instance(model, instance):\n",
    "    results = {}\n",
    "    for label in model.keys():\n",
    "        results[label] = get_prob(model, instance, label)\n",
    "    results = normalize(results)\n",
    "    best = max(results, key=results.get)\n",
    "    return (best, results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_best, test_results = classify_instance(test_model, ['a','b'])\n",
    "assert(test_best == 'b')\n",
    "assert(test_results['a'] == 0)\n",
    "assert(test_results['b'] == 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## classify\n",
    "* Takes in model as a triple nested dict, instances as a list of list of attributes, and labeled as a boolean\n",
    "* Returns a list of tuples of best labels and dict of labels to probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(model, instances, labeled=True):\n",
    "    output = []\n",
    "    for instance in instances:\n",
    "        if labeled: instance = instance[1:]\n",
    "        best, results = classify_instance(model, instance)\n",
    "        output.append((best,results))\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_instances = [['c','b'], ['a','b']]\n",
    "test_results = classify(test_model, test_instances, labeled=False)\n",
    "assert(test_results[0][1]['b'] == 0)\n",
    "assert(test_results[0][1]['a'] == 1)\n",
    "labeled_test_instances = [['a','c','b'], ['a','a','b']]\n",
    "labeled_test_results = classify(test_model, labeled_test_instances, labeled=True)\n",
    "assert(test_results == labeled_test_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## evaluate\n",
    "* takes in the dataset as a 2d np array and results as a list of predicted labels\n",
    "* Returns the error rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(data, results):\n",
    "    total = len(data)\n",
    "    errors = 0\n",
    "    for i in range(total):\n",
    "        if data[i][0] != results[i][0]:\n",
    "            errors += 1\n",
    "    return float(errors/total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(evaluate([['a','c','b'], ['a','a','b']], test_results) == 0.5)\n",
    "assert(evaluate([['a','c','b'], ['b','a','b']], test_results) == 0)\n",
    "assert(evaluate([['b','c','b'], ['a','a','b']], test_results) == 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## print_mean_variance\n",
    "* Takes in a list of error values\n",
    "* Returns the mean and variance of those error values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_mean_variance(errors):\n",
    "    mean = float(sum(errors)/len(errors))\n",
    "    print(\"Mean: {0}\".format(mean))\n",
    "    diffs = [(mean-error)**2 for error in errors]\n",
    "    variance = float(sum(diffs)/(len(errors)-1))\n",
    "    print(\"Variance: {0}\\n\".format(variance))\n",
    "    return mean, variance #returned for testing purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: 0.5\n",
      "Variance: 0.25\n",
      "\n",
      "Mean: 1.0\n",
      "Variance: 0.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_errors = [0.5, 1, 0]\n",
    "test_mean, test_variance = print_mean_variance(test_errors)\n",
    "assert(test_mean == 0.5)\n",
    "assert(test_variance == 0.25)\n",
    "test_errors = [1, 1, 1]\n",
    "test_mean, test_variance = print_mean_variance(test_errors)\n",
    "assert(test_variance == 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## cross_validate\n",
    "* takes in a dataset as 2d numpy array, smoothing as a boolean, and folds as an int\n",
    "* Returns the mean and variance of the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validate(data, smoothing, folds, test=False):\n",
    "    att_vals = get_att_vals(data)\n",
    "    np.random.shuffle(data)\n",
    "    folds = np.array_split(data, folds)\n",
    "    errors = []\n",
    "    fold_num = 0\n",
    "    for fold in folds:\n",
    "        fold_num += 1\n",
    "        split = int(0.8*(len(fold)))\n",
    "        train_set = fold[:split]\n",
    "        test_set = fold[split:]\n",
    "        model = train(train_set, att_vals, smoothing)\n",
    "        results = classify(model, test_set)\n",
    "        error = float(evaluate(test_set, results))\n",
    "        errors.append(error)\n",
    "        print(\"Fold {0} error rate: {1}\\n\".format(fold_num, error))\n",
    "    mean, variance = print_mean_variance(errors)\n",
    "    if test: return mean, variance #returns for testing purposes only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 error rate: 0.0\n",
      "\n",
      "Fold 2 error rate: 0.0\n",
      "\n",
      "Mean: 0.0\n",
      "Variance: 0.0\n",
      "\n",
      "Fold 1 error rate: 1.0\n",
      "\n",
      "Fold 2 error rate: 0.5\n",
      "\n",
      "Mean: 0.75\n",
      "Variance: 0.125\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_data=np.asarray([['a', 'a', 'b'], ['a', 'a', 'b'], ['a', 'a', 'b'], ['a', 'a', 'b'], ['a', 'a', 'b'], ['a', 'a', 'b'], ['a', 'a', 'b'],\n",
    "['a', 'a', 'b'], ['a', 'a', 'b'], ['a', 'a', 'b'], ['a', 'a', 'b'], ['a', 'a', 'b'], ['a', 'a', 'b'], ['a', 'a', 'b']])\n",
    "test_mean, test_variance = cross_validate(test_data, False, 2, True)\n",
    "assert(test_mean == 0)\n",
    "assert(test_variance == 0)\n",
    "test_data=np.asarray([['a', 'b', 'a'], ['a', 'a', 'b'], ['b', 'a', 'b'], ['a', 'a', 'b'], ['a', 'a', 'a'], ['b', 'b', 'b'], ['a', 'b', 'b'],\n",
    "['b', 'a', 'a'], ['b', 'b', 'a'], ['b', 'a', 'b'], ['a', 'a', 'a'], ['b', 'a', 'a'], ['a', 'b', 'b'], ['b', 'b', 'b']])\n",
    "test_mean, test_variance = cross_validate(test_data, False, 2, True)\n",
    "assert(test_mean != 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes without +1 Smoothing:\n",
      "\n",
      "Fold 1 error rate: 0.4166666666666667\n",
      "\n",
      "Fold 2 error rate: 0.25\n",
      "\n",
      "Fold 3 error rate: 0.36363636363636365\n",
      "\n",
      "Fold 4 error rate: 0.45454545454545453\n",
      "\n",
      "Fold 5 error rate: 0.45454545454545453\n",
      "\n",
      "Mean: 0.3878787878787879\n",
      "Variance: 0.007328971533516988\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Naive Bayes without +1 Smoothing:\\n\")\n",
    "cross_validate(dataset, False, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes with +1 Smoothing:\n",
      "\n",
      "Fold 1 error rate: 0.25\n",
      "\n",
      "Fold 2 error rate: 0.4166666666666667\n",
      "\n",
      "Fold 3 error rate: 0.36363636363636365\n",
      "\n",
      "Fold 4 error rate: 0.36363636363636365\n",
      "\n",
      "Fold 5 error rate: 0.09090909090909091\n",
      "\n",
      "Mean: 0.296969696969697\n",
      "Variance: 0.016970844811753906\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Naive Bayes with +1 Smoothing:\\n\")\n",
    "cross_validate(dataset, True, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Citations:\n",
    "\n",
    "[1] Zwitter,Matjaz and Soklic,Milan. (1988). Breast Cancer. UCI Machine Learning Repository. https://doi.org/10.24432/C51P4M."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (en605645)",
   "language": "python",
   "name": "en605645"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "81px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
